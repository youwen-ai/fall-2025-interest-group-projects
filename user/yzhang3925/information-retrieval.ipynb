{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval: Dense Retrieval with FAISS and Neural Reranking\n",
    "\n",
    "Welcome to this comprehensive tutorial on modern information retrieval! In this notebook, we'll explore:\n",
    "\n",
    "1. **MS MARCO Dataset** - Real web search queries with relevance judgments\n",
    "2. **Dense Embeddings** - Neural representations for semantic search\n",
    "3. **FAISS** - Fast similarity search for first-stage retrieval\n",
    "4. **BM25** - Traditional keyword-based retrieval baseline\n",
    "5. **Neural Reranking** - CrossEncoders for accurate result refinement\n",
    "6. **PyTerrier** - Pipeline composition and experimentation framework\n",
    "7. **Complete Comparison** - Sparse, Dense, and Hybrid retrieval methods\n",
    "8. **Evaluation Metrics** - MRR, Recall, Precision for measuring quality\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Two-Stage Retrieval Pipeline\n",
    "\n",
    "**Stage 1**: Fast candidate retrieval (FAISS/BM25) → top-100 results  \n",
    "**Stage 2**: Accurate reranking (CrossEncoder) → final top-10\n",
    "\n",
    "This is how **Google, Bing, and all modern search engines work!**\n",
    "\n",
    "By the end, you'll understand how to build production-ready IR systems. Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set JAVA_HOME for PyTerrier\n",
    "os.environ[\"JAVA_HOME\"] = os.path.expanduser(\"~/bin/jdk-17.0.2\")\n",
    "os.environ[\"PATH\"] = (\n",
    "    os.path.join(os.environ[\"JAVA_HOME\"], \"bin\") + \":\" + os.environ.get(\"PATH\", \"\")\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "print(f\"javac location: {os.path.join(os.environ['JAVA_HOME'], 'bin', 'javac')}\")\n",
    "\n",
    "# Check if javac exists\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"JAVA VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Check java version\n",
    "    result_java = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n",
    "    print(\"\\njava version:\")\n",
    "    print(result_java.stderr.strip())\n",
    "except Exception as e:\n",
    "    print(f\"Error finding java: {e}\")\n",
    "\n",
    "try:\n",
    "    # Check javac version\n",
    "    result_javac = subprocess.run([\"javac\", \"-version\"], capture_output=True, text=True)\n",
    "    print(\"\\njavac version:\")\n",
    "    # javac outputs to stderr\n",
    "    print(\n",
    "        result_javac.stderr.strip()\n",
    "        if result_javac.stderr\n",
    "        else result_javac.stdout.strip()\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error finding javac: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "import pyterrier as pt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Initialize PyTerrier (required before use)\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The MS MARCO Dataset\n",
    "\n",
    "**MS MARCO** (Microsoft MAchine Reading COmprehension) is one of the most important datasets in modern information retrieval.\n",
    "\n",
    "#### Key Facts:\n",
    "- **Queries**: Real anonymized Bing search queries\n",
    "- **Passages**: ~8.8 million web passages from Bing search results\n",
    "- **Relevance**: Human-annotated relevant passages for each query\n",
    "- **Task**: Given a query, retrieve the most relevant passages\n",
    "\n",
    "#### Why MS MARCO?\n",
    "\n",
    "1. **Realistic**: Real user queries, not synthetic\n",
    "2. **Large-scale**: Millions of passages to search through\n",
    "3. **Industry standard**: Used by Google, Microsoft, Meta for IR research\n",
    "4. **Benchmark**: All modern retrieval systems are evaluated on MS MARCO\n",
    "\n",
    "#### Example:\n",
    "- **Query**: \"what is the difference between a tornado and a hurricane\"\n",
    "- **Relevant Passage**: \"The main difference between a tornado and a hurricane is that a tornado forms over land while a hurricane forms over water. Tornadoes are smaller but more intense, while hurricanes are larger but less intense per area...\"\n",
    "\n",
    "For this tutorial, we'll work with a **sampled subset** to keep things fast and interactive!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding MS MARCO Dataset Structure\n",
    "\n",
    "### What is MS MARCO?\n",
    "\n",
    "MS MARCO contains **real Bing search queries** paired with **relevant passages** from web pages.\n",
    "\n",
    "Each item in the dataset has:\n",
    "- **`query`**: A search question (e.g., \"how do airplanes fly\")\n",
    "- **`passages`**: Multiple candidate passages that might answer the query\n",
    "- **`is_selected`**: Labels showing which passages are relevant\n",
    "\n",
    "### Our Strategy\n",
    "\n",
    "We'll load the dataset **twice** for different purposes:\n",
    "\n",
    "1. **Corpus (10K items)** → Extract **passages** → Documents to search through\n",
    "2. **Queries (500 items)** → Extract **queries** → Search questions to test\n",
    "\n",
    "Think of it like a library:\n",
    "- **Corpus** = 10,000 books on shelves (documents to search)\n",
    "- **Queries** = 500 questions from readers (searches to perform)\n",
    "\n",
    "Let's load both and inspect the structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MS MARCO dataset...\")\n",
    "print(\"This may take a few minutes on first run (dataset will be cached)...\\n\")\n",
    "\n",
    "# Sample sizes\n",
    "CORPUS_SIZE = 10000  # Items to extract PASSAGES from (documents to search)\n",
    "QUERY_SIZE = 500  # Items to extract QUERIES from (search questions)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING STRATEGY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Corpus: {CORPUS_SIZE:,} items → Extract PASSAGES (documents)\")\n",
    "print(f\"Queries: {QUERY_SIZE:,} items → Extract QUERIES (search questions)\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Load corpus items (we'll extract passages from these)\n",
    "print(f\"Loading {CORPUS_SIZE:,} items for corpus...\")\n",
    "corpus_dataset = load_dataset(\"ms_marco\", \"v1.1\", split=f\"train[:{CORPUS_SIZE}]\")\n",
    "\n",
    "# Load query items (we'll extract queries from these)\n",
    "print(f\"Loading {QUERY_SIZE:,} items for queries...\")\n",
    "query_dataset = load_dataset(\"ms_marco\", \"v1.1\", split=f\"train[:{QUERY_SIZE}]\")\n",
    "\n",
    "print(\"\\nDataset loaded!\")\n",
    "print(f\"Corpus items: {len(corpus_dataset):,}\")\n",
    "print(f\"Query items: {len(query_dataset):,}\")\n",
    "print(\"\\nNext: We'll extract the actual text from these items...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataset structure\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET STRUCTURE INSPECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show one item from corpus_dataset\n",
    "print(\"\\nSAMPLE ITEM FROM CORPUS DATASET:\")\n",
    "print(\"-\" * 80)\n",
    "sample_corpus_item = corpus_dataset[0]\n",
    "\n",
    "print(f\"Keys in item: {list(sample_corpus_item.keys())}\\n\")\n",
    "\n",
    "print(\"1. Query field:\")\n",
    "print(f\"Type: {type(sample_corpus_item['query'])}\")\n",
    "print(f\"Content: '{sample_corpus_item['query'][:100]}...'\\n\")\n",
    "\n",
    "print(\"2. Passages field:\")\n",
    "print(f\"Type: {type(sample_corpus_item['passages'])}\")\n",
    "print(f\"Keys: {list(sample_corpus_item['passages'].keys())}\\n\")\n",
    "if isinstance(sample_corpus_item[\"passages\"], dict):\n",
    "    passage_texts = sample_corpus_item[\"passages\"][\"passage_text\"]\n",
    "    print(f\"Number of passages: {len(passage_texts)}\")\n",
    "    print(f\"First passage: '{passage_texts[0][:150]}...'\\n\")\n",
    "\n",
    "    if \"is_selected\" in sample_corpus_item[\"passages\"]:\n",
    "        is_selected = sample_corpus_item[\"passages\"][\"is_selected\"]\n",
    "        print(f\"Relevance labels: {is_selected}\")\n",
    "        print(f\"Relevant passages: {sum(is_selected)} out of {len(is_selected)}\\n\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nSAMPLE ITEM FROM QUERY DATASET:\")\n",
    "print(\"-\" * 80)\n",
    "sample_query_item = query_dataset[0]\n",
    "\n",
    "print(f\"Query: '{sample_query_item['query']}'\\n\")\n",
    "\n",
    "if \"passages\" in sample_query_item:\n",
    "    query_passages = sample_query_item[\"passages\"]\n",
    "    if isinstance(query_passages, dict) and \"passage_text\" in query_passages:\n",
    "        print(f\"Number of candidate passages: {len(query_passages['passage_text'])}\")\n",
    "        print(f\"First candidate: '{query_passages['passage_text'][0][:150]}...'\\n\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual representation of what we're doing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Title\n",
    "ax.text(\n",
    "    5,\n",
    "    9.5,\n",
    "    \"MS MARCO Dataset Processing Flow\",\n",
    "    fontsize=20,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    ")\n",
    "\n",
    "# MS MARCO Dataset box\n",
    "dataset_box = FancyBboxPatch(\n",
    "    (0.5, 7),\n",
    "    9,\n",
    "    1.5,\n",
    "    boxstyle=\"round,pad=0.1\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"lightblue\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.add_patch(dataset_box)\n",
    "ax.text(\n",
    "    5,\n",
    "    8.2,\n",
    "    \"MS MARCO Dataset (train split)\",\n",
    "    fontsize=14,\n",
    "    ha=\"center\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.text(\n",
    "    5,\n",
    "    7.6,\n",
    "    \"Each item contains: query + passages + labels\",\n",
    "    fontsize=11,\n",
    "    ha=\"center\",\n",
    "    style=\"italic\",\n",
    ")\n",
    "\n",
    "# Split into two paths\n",
    "# Left path - Corpus\n",
    "corpus_box = FancyBboxPatch(\n",
    "    (0.5, 4.5),\n",
    "    4,\n",
    "    1.8,\n",
    "    boxstyle=\"round,pad=0.1\",\n",
    "    edgecolor=\"green\",\n",
    "    facecolor=\"lightgreen\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.add_patch(corpus_box)\n",
    "ax.text(2.5, 5.9, \"corpus_dataset\", fontsize=13, ha=\"center\", fontweight=\"bold\")\n",
    "ax.text(2.5, 5.5, \"100,000 items\", fontsize=11, ha=\"center\")\n",
    "ax.text(2.5, 5.1, \"Extract: passages field\", fontsize=10, ha=\"center\", style=\"italic\")\n",
    "ax.text(2.5, 4.7, \"→ Documents to search\", fontsize=10, ha=\"center\", color=\"darkgreen\")\n",
    "\n",
    "# Right path - Queries\n",
    "query_box = FancyBboxPatch(\n",
    "    (5.5, 4.5),\n",
    "    4,\n",
    "    1.8,\n",
    "    boxstyle=\"round,pad=0.1\",\n",
    "    edgecolor=\"orange\",\n",
    "    facecolor=\"lightyellow\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.add_patch(query_box)\n",
    "ax.text(7.5, 5.9, \"query_dataset\", fontsize=13, ha=\"center\", fontweight=\"bold\")\n",
    "ax.text(7.5, 5.5, \"5,000 items\", fontsize=11, ha=\"center\")\n",
    "ax.text(7.5, 5.1, \"Extract: query field\", fontsize=10, ha=\"center\", style=\"italic\")\n",
    "ax.text(7.5, 4.7, \"→ Search questions\", fontsize=10, ha=\"center\", color=\"darkorange\")\n",
    "\n",
    "# Arrows from dataset to splits\n",
    "arrow1 = FancyArrowPatch(\n",
    "    (2.5, 7), (2.5, 6.3), arrowstyle=\"->\", mutation_scale=30, linewidth=2, color=\"green\"\n",
    ")\n",
    "ax.add_patch(arrow1)\n",
    "\n",
    "arrow2 = FancyArrowPatch(\n",
    "    (7.5, 7),\n",
    "    (7.5, 6.3),\n",
    "    arrowstyle=\"->\",\n",
    "    mutation_scale=30,\n",
    "    linewidth=2,\n",
    "    color=\"orange\",\n",
    ")\n",
    "ax.add_patch(arrow2)\n",
    "\n",
    "# Processed outputs\n",
    "passages_box = FancyBboxPatch(\n",
    "    (0.5, 2),\n",
    "    4,\n",
    "    1.5,\n",
    "    boxstyle=\"round,pad=0.1\",\n",
    "    edgecolor=\"darkgreen\",\n",
    "    facecolor=\"#90EE90\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.add_patch(passages_box)\n",
    "ax.text(2.5, 3.2, \"passages list\", fontsize=12, ha=\"center\", fontweight=\"bold\")\n",
    "ax.text(2.5, 2.8, \"~100K text strings\", fontsize=10, ha=\"center\")\n",
    "ax.text(\n",
    "    2.5,\n",
    "    2.4,\n",
    "    '[\"passage 1\", \"passage 2\", ...]',\n",
    "    fontsize=9,\n",
    "    ha=\"center\",\n",
    "    family=\"monospace\",\n",
    ")\n",
    "\n",
    "queries_box = FancyBboxPatch(\n",
    "    (5.5, 2),\n",
    "    4,\n",
    "    1.5,\n",
    "    boxstyle=\"round,pad=0.1\",\n",
    "    edgecolor=\"darkorange\",\n",
    "    facecolor=\"#FFE5B4\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.add_patch(queries_box)\n",
    "ax.text(7.5, 3.2, \"queries list\", fontsize=12, ha=\"center\", fontweight=\"bold\")\n",
    "ax.text(7.5, 2.8, \"5K text strings\", fontsize=10, ha=\"center\")\n",
    "ax.text(\n",
    "    7.5, 2.4, '[\"query 1\", \"query 2\", ...]', fontsize=9, ha=\"center\", family=\"monospace\"\n",
    ")\n",
    "\n",
    "# Arrows to processed\n",
    "arrow3 = FancyArrowPatch(\n",
    "    (2.5, 4.5),\n",
    "    (2.5, 3.5),\n",
    "    arrowstyle=\"->\",\n",
    "    mutation_scale=30,\n",
    "    linewidth=2,\n",
    "    color=\"darkgreen\",\n",
    ")\n",
    "ax.add_patch(arrow3)\n",
    "\n",
    "arrow4 = FancyArrowPatch(\n",
    "    (7.5, 4.5),\n",
    "    (7.5, 3.5),\n",
    "    arrowstyle=\"->\",\n",
    "    mutation_scale=30,\n",
    "    linewidth=2,\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "ax.add_patch(arrow4)\n",
    "\n",
    "# Final use case\n",
    "use_box = FancyBboxPatch(\n",
    "    (2, 0.3),\n",
    "    6,\n",
    "    1,\n",
    "    boxstyle=\"round,pad=0.1\",\n",
    "    edgecolor=\"purple\",\n",
    "    facecolor=\"lavender\",\n",
    "    linewidth=3,\n",
    ")\n",
    "ax.add_patch(use_box)\n",
    "ax.text(\n",
    "    5, 0.95, \"Information Retrieval System\", fontsize=13, ha=\"center\", fontweight=\"bold\"\n",
    ")\n",
    "ax.text(\n",
    "    5,\n",
    "    0.55,\n",
    "    \"For each query → Search through passages → Return most relevant\",\n",
    "    fontsize=10,\n",
    "    ha=\"center\",\n",
    ")\n",
    "\n",
    "# Arrows to final use\n",
    "arrow5 = FancyArrowPatch(\n",
    "    (2.5, 2),\n",
    "    (3.5, 1.3),\n",
    "    arrowstyle=\"->\",\n",
    "    mutation_scale=25,\n",
    "    linewidth=2,\n",
    "    color=\"purple\",\n",
    ")\n",
    "ax.add_patch(arrow5)\n",
    "\n",
    "arrow6 = FancyArrowPatch(\n",
    "    (7.5, 2),\n",
    "    (6.5, 1.3),\n",
    "    arrowstyle=\"->\",\n",
    "    mutation_scale=25,\n",
    "    linewidth=2,\n",
    "    color=\"purple\",\n",
    ")\n",
    "ax.add_patch(arrow6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Takeaway:\")\n",
    "print(\"   • We load the SAME dataset twice (different samples)\")\n",
    "print(\"   • From corpus items: Extract passages → What to search\")\n",
    "print(\"   • From query items: Extract queries → What to search for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Relevance: Not All Passages Are Relevant!\n",
    "\n",
    "### Important Concept\n",
    "\n",
    "In MS MARCO, each query comes with **multiple candidate passages**, but **most are NOT relevant**!\n",
    "\n",
    "### Example Structure\n",
    "\n",
    "Each item looks like this:\n",
    "\n",
    "- **Query**: \"how do airplanes fly\"\n",
    "- **Passages**: 3 candidate passages\n",
    "- **`is_selected`**: [1, 0, 0] - only the FIRST one is relevant!\n",
    "\n",
    "| Passage | Text | `is_selected` | Status |\n",
    "|---------|------|-------------|---------|\n",
    "| 0 | \"Airplanes fly due to lift...\" | 1 | ✅ RELEVANT |\n",
    "| 1 | \"Book tickets online...\" | 0 | ❌ NOT RELEVANT |\n",
    "| 2 | \"Maintenance guide...\" | 0 | ❌ NOT RELEVANT |\n",
    "\n",
    "### The `is_selected` Field\n",
    "\n",
    "- **1** = This passage answers the query ✅ RELEVANT\n",
    "- **0** = This passage does NOT answer the query ❌ NOT RELEVANT\n",
    "\n",
    "**Only passages with `is_selected`=1 are actually relevant to the query!**\n",
    "\n",
    "### What are Qrels?\n",
    "\n",
    "**Qrels** (Query Relevance judgments) = A mapping that stores which documents are relevant for each query.\n",
    "\n",
    "We extract qrels to track: \"For query X, documents Y and Z are relevant\"\n",
    "\n",
    "### Why Do We Need Qrels?\n",
    "\n",
    "**To evaluate our retrieval system later!**\n",
    "\n",
    "Qrels let us answer:\n",
    "- Did our system retrieve the relevant passages?\n",
    "- How good is our ranking?\n",
    "\n",
    "We'll extract qrels now and use them for evaluation metrics later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract passages and queries with CONSISTENT ID scheme\n",
    "passages = []\n",
    "passage_ids = []\n",
    "\n",
    "print(\"Processing passages...\")\n",
    "\n",
    "# MS MARCO v1.1 structure: item['passages'] is a dict with keys: passage_text, is_selected, url\n",
    "for idx, item in enumerate(tqdm(corpus_dataset, desc=\"Loading passages\")):\n",
    "    if \"passages\" in item and item[\"passages\"]:\n",
    "        passages_dict = item[\"passages\"]\n",
    "\n",
    "        if isinstance(passages_dict, dict) and \"passage_text\" in passages_dict:\n",
    "            passage_texts = passages_dict[\"passage_text\"]\n",
    "\n",
    "            if isinstance(passage_texts, list):\n",
    "                for p_idx, text in enumerate(passage_texts):\n",
    "                    if text:\n",
    "                        passages.append(text)\n",
    "                        # KEY CHANGE: Use format that matches qrels\n",
    "                        passage_ids.append(f\"item_{idx}_p{p_idx}\")  # Consistent format!\n",
    "            else:\n",
    "                if passage_texts:\n",
    "                    passages.append(passage_texts)\n",
    "                    passage_ids.append(f\"item_{idx}_p0\")\n",
    "\n",
    "print(f\"\\nLoaded {len(passages):,} passages\")\n",
    "\n",
    "# Process queries with MATCHING ID scheme\n",
    "queries = []\n",
    "query_ids = []\n",
    "qrels = defaultdict(dict)\n",
    "\n",
    "print(\"Processing queries...\")\n",
    "for idx, item in enumerate(query_dataset):\n",
    "    query_text = item.get(\"query\", \"\")\n",
    "    if query_text:\n",
    "        queries.append(query_text)\n",
    "        qid = f\"q_{idx}\"\n",
    "        query_ids.append(qid)\n",
    "\n",
    "        # Store relevance judgments with MATCHING IDs\n",
    "        if \"passages\" in item and isinstance(item[\"passages\"], dict):\n",
    "            if \"is_selected\" in item[\"passages\"]:\n",
    "                is_selected = item[\"passages\"][\"is_selected\"]\n",
    "                if isinstance(is_selected, list):\n",
    "                    for p_idx, selected in enumerate(is_selected):\n",
    "                        if selected == 1:\n",
    "                            # KEY CHANGE: Use same format as passage_ids\n",
    "                            doc_id = f\"item_{idx}_p{p_idx}\"\n",
    "                            qrels[qid][doc_id] = 1\n",
    "\n",
    "print(\"\\nProcessed:\")\n",
    "print(f\"Passages: {len(passages):,}\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(f\"Queries with relevance judgments: {len(qrels):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample queries and passages\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE QUERIES AND PASSAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "num_samples = 5\n",
    "sample_indices = random.sample(range(len(queries)), min(num_samples, len(queries)))\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Query ID: {query_ids[idx]}\")\n",
    "    print(f\"Query: {queries[idx]}\")\n",
    "    print(f\"Length: {len(queries[idx])} characters\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE PASSAGES FROM CORPUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_passages = random.sample(range(len(passages)), min(5, len(passages)))\n",
    "for i, idx in enumerate(sample_passages, 1):\n",
    "    passage_text = (\n",
    "        passages[idx][:200] + \"...\" if len(passages[idx]) > 200 else passages[idx]\n",
    "    )\n",
    "    print(f\"\\nPassage {i}:\")\n",
    "    print(f\"ID: {passage_ids[idx]}\")\n",
    "    print(f\"Text: {passage_text}\")\n",
    "    print(f\"Length: {len(passages[idx])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why Dataset Statistics Matter\n",
    "\n",
    "### Understanding Query & Passage Lengths\n",
    "\n",
    "Knowing the **length** of queries and passages is crucial for several reasons:\n",
    "\n",
    "#### 1. Model Constraints\n",
    "\n",
    "**Embedding models have maximum sequence lengths:**\n",
    "- Our model: `multi-qa-MiniLM-L6` has max length = **512 tokens**\n",
    "- If passages are longer → they get **truncated** (information lost!)\n",
    "- If most passages are 1000 chars but model only handles 512 tokens → problem!\n",
    "\n",
    "**Example:**\n",
    "- Average passage: 600 characters ≈ 150 words ≈ 200 tokens ✅ Fits!\n",
    "- Long passage: 3000 characters → Gets cut off ❌ Loses information\n",
    "\n",
    "#### 2. Performance & Speed\n",
    "\n",
    "**Longer texts = Slower processing:**\n",
    "- Embedding 100-word passages: Fast\n",
    "- Embedding 1000-word passages: 10x slower!\n",
    "\n",
    "Knowing average length helps us estimate:\n",
    "- How long will encoding take?\n",
    "- Do we need batch processing?\n",
    "- Should we split long passages?\n",
    "\n",
    "#### 3. Understanding Search Behavior\n",
    "\n",
    "**Query length tells us about search patterns:**\n",
    "- Short queries (2-3 words): \"airplane flight\" → Keyword search\n",
    "- Long queries (10+ words): \"how do airplanes generate enough lift to stay in the air\" → Natural questions\n",
    "\n",
    "This helps us choose the right model and approach!\n",
    "\n",
    "#### 4. Quality Control\n",
    "\n",
    "**Statistics reveal data issues:**\n",
    "- Passages with 0 characters? → Data cleaning needed\n",
    "- Queries with 500+ characters? → Probably spam or errors\n",
    "- Very short passages (< 10 chars)? → Not useful for retrieval\n",
    "\n",
    "#### 5. Comparison Baseline\n",
    "\n",
    "**Know what's \"normal\" for your dataset:**\n",
    "- If retrieval suddenly slows down → Check if new data has longer passages\n",
    "- If accuracy drops → Maybe new queries are much longer/shorter than training data\n",
    "\n",
    "**Without stats:** You might pick a model that's too small or waste money on one that's too large!\n",
    "\n",
    "### Bottom Line\n",
    "\n",
    "Dataset statistics = Understanding what you're working with before investing compute time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset statistics\n",
    "query_lengths = [len(q) for q in queries]\n",
    "passage_lengths = [len(p) for p in passages]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Query length distribution\n",
    "axes[0].hist(query_lengths, bins=50, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[0].set_xlabel(\"Query Length (characters)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[0].set_title(\"Query Length Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].axvline(\n",
    "    np.mean(query_lengths),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {np.mean(query_lengths):.1f}\",\n",
    ")\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Passage length distribution\n",
    "axes[1].hist(passage_lengths, bins=50, color=\"coral\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[1].set_xlabel(\"Passage Length (characters)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[1].set_title(\"Passage Length Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].axvline(\n",
    "    np.mean(passage_lengths),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {np.mean(passage_lengths):.1f}\",\n",
    ")\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(\n",
    "    f\"Avg query length: {np.mean(query_lengths):.1f} chars ({np.mean([len(q.split()) for q in queries]):.1f} words)\"\n",
    ")\n",
    "print(\"\\nPassages: {len(passages):,}\")\n",
    "print(\n",
    "    f\"Avg passage length: {np.mean(passage_lengths):.1f} chars ({np.mean([len(p.split()) for p in passages]):.1f} words)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dense Embeddings for Semantic Search\n",
    "\n",
    "### The Problem with Keyword Search\n",
    "\n",
    "Traditional search (BM25, TF-IDF) relies on **exact word matching**:\n",
    "- Query: \"automobile repair\"\n",
    "- Won't match: \"car maintenance\" (same meaning, different words!)\n",
    "- Will match: \"automobile sales\" (same words, different meaning!)\n",
    "\n",
    "### Enter Dense Embeddings\n",
    "\n",
    "**Dense embeddings** convert text into vectors that capture **semantic meaning**:\n",
    "- Similar meanings → Similar vectors\n",
    "- \"automobile repair\" and \"car maintenance\" will have high similarity\n",
    "- Works across synonyms, paraphrases, and languages\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Encode** queries and passages into dense vectors (e.g., 384 or 768 dimensions)\n",
    "2. **Measure similarity** using cosine similarity or dot product\n",
    "3. **Retrieve** passages with highest similarity to query\n",
    "\n",
    "### Model Choice: Sentence Transformers\n",
    "\n",
    "We'll use **Sentence-BERT** models, specifically designed for semantic search:\n",
    "- Pre-trained on millions of question-passage pairs\n",
    "- Optimized for retrieval tasks\n",
    "- Fast inference\n",
    "\n",
    "Popular models:\n",
    "- [`multi-qa-MiniLM-L6-cos-v1`](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) - Small, fast (384 dim) -- Good for teaching\n",
    "- [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) - General purpose (384 dim)\n",
    "- [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) - High quality (768 dim)\n",
    "\n",
    "Let's load our embedding model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model for embeddings\n",
    "# This model is specifically trained for question-answering retrieval!\n",
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "print(f\"Loading embedding model: {model_name}\")\n",
    "print(\"This may take a minute on first run...\\n\")\n",
    "\n",
    "# Load model\n",
    "embedding_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"MODEL INFORMATION\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# Model info\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Embedding Dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"Max Sequence Length: {embedding_model.max_seq_length} tokens\")\n",
    "print(f\"Device: {embedding_model.device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in embedding_model.parameters())\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Model Size: ~{total_params * 4 / (1024**2):.1f} MB\")\n",
    "\n",
    "print(f\"{'=' * 60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the embedding model with sample texts\n",
    "test_query = \"how do airplanes fly\"\n",
    "test_passages = [\n",
    "    \"Airplanes fly because of lift generated by air flowing over wings.\",\n",
    "    \"The principles of aerodynamics explain how aircraft achieve flight.\",\n",
    "    \"Pizza is a popular Italian food with cheese and tomato sauce.\",\n",
    "    \"Python is a programming language used for data science.\",\n",
    "]\n",
    "\n",
    "print(f\"Test Query: '{test_query}'\\n\")\n",
    "\n",
    "# Encode\n",
    "query_emb = embedding_model.encode([test_query], convert_to_tensor=True)\n",
    "passage_embs = embedding_model.encode(test_passages, convert_to_tensor=True)\n",
    "\n",
    "# Compute similarities\n",
    "similarities = torch.nn.functional.cosine_similarity(query_emb, passage_embs)\n",
    "\n",
    "print(\"Similarity Scores (higher = more relevant):\\n\")\n",
    "for i, (passage, score) in enumerate(\n",
    "    sorted(zip(test_passages, similarities), key=lambda x: x[1], reverse=True), 1\n",
    "):\n",
    "    bar = \"█\" * int(score * 50)\n",
    "    print(f\"{i}. Score: {score:.4f} {bar}\")\n",
    "    print(f\"   {passage}\\n\")\n",
    "\n",
    "print(\n",
    "    \"Notice: Semantically related passages get higher scores, even without word overlap!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all passages into embeddings\n",
    "# This is the computationally expensive part! will take ~1min on GPU and much longer on CPU...\n",
    "print(f\"Encoding {len(passages):,} passages...\")\n",
    "\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "\n",
    "# Create passage embeddings\n",
    "passage_embeddings = embedding_model.encode(\n",
    "    passages,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=False,  # Return numpy for FAISS\n",
    "    normalize_embeddings=True,  # Normalize for cosine similarity\n",
    ")\n",
    "print(\"\\nPassage embeddings created!\")\n",
    "print(f\"Shape: {passage_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all queries into embeddings\n",
    "print(f\"Encoding {len(queries):,} queries...\")\n",
    "\n",
    "query_embeddings = embedding_model.encode(\n",
    "    queries,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=False,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "print(\"\\nQuery embeddings created!\")\n",
    "print(f\"Shape: {query_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FAISS: Fast Similarity Search - Stage 1\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "We have:\n",
    "- 100,000 passages (or millions in real systems!)\n",
    "- Each query needs to search ALL passages\n",
    "- Computing similarity for each pair is **slow**: O(n) per query\n",
    "\n",
    "For 1 query × 100K passages = 100K similarity computations\n",
    "For 1000 queries = **100 million** computations!\n",
    "\n",
    "### Enter FAISS\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) is a library for:\n",
    "- **Fast** approximate nearest neighbor search\n",
    "- **Scalable** to billions of vectors\n",
    "- **Optimized** for GPUs\n",
    "- Used in production at Meta, Google, etc.\n",
    "\n",
    "### How FAISS Works\n",
    "\n",
    "1. **Index Building**: Organizes vectors for fast search\n",
    "2. **Query**: Returns top-K most similar vectors in milliseconds\n",
    "3. **Trade-off**: Speed vs accuracy (we'll use exact search for teaching)\n",
    "\n",
    "### Types of FAISS Indexes\n",
    "\n",
    "- `IndexFlatL2`: Exact search (L2 distance) - Slow but accurate\n",
    "- `IndexFlatIP`: Exact search (inner product/cosine) - What we'll use!\n",
    "- `IndexIVFFlat`: Approximate search - Much faster for large scale\n",
    "- `IndexHNSW`: Graph-based approximate search - Best accuracy/speed trade-off\n",
    "\n",
    "For our 10K passages, exact search is fine. For millions, we'd use approximate methods.\n",
    "\n",
    "Let's build our FAISS index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building FAISS index...\")\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dim = passage_embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index\n",
    "# IndexFlatIP = Flat index with Inner Product (cosine similarity for normalized vectors)\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "\n",
    "# Add passage embeddings to index\n",
    "index.add(passage_embeddings.astype(\"float32\"))\n",
    "\n",
    "print(\"FAISS index built!\")\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"INDEX INFORMATION\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(\"Index Type: Flat (Exact Search)\")\n",
    "print(\"Similarity Metric: Inner Product (Cosine for normalized vectors)\")\n",
    "print(f\"Embedding Dimension: {embedding_dim}\")\n",
    "print(f\"Number of Vectors: {index.ntotal:,}\")\n",
    "print(f\"Index Size: ~{passage_embeddings.nbytes / (1024**2):.1f} MB\")\n",
    "print(f\"Is Trained: {index.is_trained}\")\n",
    "print(f\"{'=' * 60}\\n\")\n",
    "print(\"The index is now ready for lightning-fast retrieval!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform first-stage retrieval\n",
    "K = 100  # Retrieve top-100 passages per query\n",
    "\n",
    "print(\"Performing first-stage retrieval...\")\n",
    "print(f\"Retrieving top-{K} passages for {len(queries):,} queries...\\n\")\n",
    "\n",
    "# Search\n",
    "scores, indices = index.search(query_embeddings.astype(\"float32\"), K)\n",
    "\n",
    "print(\"Retrieval complete!\")\n",
    "print(f\"Results shape: {indices.shape}\")\n",
    "print(f\"Scores shape: {scores.shape}\")\n",
    "print(f\"Total retrievals: {indices.shape[0] * indices.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample retrieval results\n",
    "num_examples = 3\n",
    "sample_query_indices = random.sample(range(len(queries)), num_examples)\n",
    "\n",
    "for example_num, qidx in enumerate(sample_query_indices, 1):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"EXAMPLE {example_num}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nQuery: {queries[qidx]}\\n\")\n",
    "    print(\"Top-5 Retrieved Passages:\\n\")\n",
    "\n",
    "    for rank, (score, pidx) in enumerate(zip(scores[qidx][:5], indices[qidx][:5]), 1):\n",
    "        passage_text = passages[pidx]\n",
    "        # Truncate long passages\n",
    "        if len(passage_text) > 150:\n",
    "            passage_text = passage_text[:150] + \"...\"\n",
    "        print(f\"   Rank {rank} | Score: {score:.4f}\")\n",
    "        print(f\"   {passage_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze score distributions\n",
    "all_scores = scores.flatten()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Score distribution\n",
    "axes[0].hist(all_scores, bins=50, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[0].set_xlabel(\"Similarity Score\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[0].set_title(\"Distribution of Retrieval Scores\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].axvline(\n",
    "    np.mean(all_scores),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {np.mean(all_scores):.3f}\",\n",
    ")\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Top-1 scores vs rank\n",
    "top_scores = scores[:, 0]  # Best score for each query\n",
    "axes[1].hist(top_scores, bins=50, color=\"coral\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[1].set_xlabel(\"Top-1 Similarity Score\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[1].set_title(\"Distribution of Best Match Scores\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].axvline(\n",
    "    np.mean(top_scores),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {np.mean(top_scores):.3f}\",\n",
    ")\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRetrieval Statistics:\")\n",
    "print(f\"Average similarity score: {np.mean(all_scores):.4f}\")\n",
    "print(f\"Average top-1 score: {np.mean(top_scores):.4f}\")\n",
    "print(f\"Min score: {np.min(all_scores):.4f}\")\n",
    "print(f\"Max score: {np.max(all_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyzing Retrieval Score Distributions\n",
    "\n",
    "### What Are We Looking At?\n",
    "\n",
    "After FAISS retrieval, each query-passage pair has a **similarity score** (higher = more similar).\n",
    "\n",
    "We're analyzing two distributions:\n",
    "\n",
    "#### 1. All Retrieval Scores (Left Plot)\n",
    "- **What**: All 500,000 similarity scores (5K queries × 100 retrieved passages each)\n",
    "- **Why**: Shows the overall quality of matches\n",
    "- **Good sign**: Most scores clustered around a reasonable range\n",
    "- **Bad sign**: All scores near 0 (model isn't finding similarities)\n",
    "\n",
    "#### 2. Top-1 Scores (Right Plot)\n",
    "- **What**: Only the BEST match score for each query\n",
    "- **Why**: Shows how confident FAISS is about top results\n",
    "- **Good sign**: High top-1 scores (model is confident)\n",
    "- **Bad sign**: Low top-1 scores (even best matches are weak)\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "**Healthy retrieval:**\n",
    "- Mean score: 0.3 - 0.7 (cosine similarity)\n",
    "- Top-1 scores higher than average scores\n",
    "- Clear separation between good and bad matches\n",
    "\n",
    "**Poor retrieval:**\n",
    "- All scores near 0 or 1 (no discrimination)\n",
    "- Top-1 scores same as average (no clear winners)\n",
    "\n",
    "This helps us understand if FAISS is finding meaningful similarities!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Neural Reranking: Stage 2\n",
    "\n",
    "### Why Rerank?\n",
    "\n",
    "**First-stage retrieval** (FAISS) is:\n",
    "- ✅ Fast: Can search millions in milliseconds\n",
    "- ✅ Scalable: Efficient for large corpora\n",
    "- ❌ Less accurate: Uses only vector similarity\n",
    "\n",
    "**Problem**: The bi-encoder (separate query/passage encodings) can miss subtle relevance signals.\n",
    "\n",
    "### Enter Cross-Encoders\n",
    "\n",
    "**Cross-encoders** (rerankers) are:\n",
    "- More accurate: Process query + passage together\n",
    "- Capture interactions between query and passage words\n",
    "- See both texts simultaneously\n",
    "- But slower: Must encode each query-passage pair\n",
    "\n",
    "### Best of Both Worlds!\n",
    "\n",
    "- **FAISS**: Eliminates 99.99% of irrelevant documents quickly\n",
    "- **Reranker**: Accurately ranks the remaining candidates\n",
    "\n",
    "### Models We'll Use\n",
    "\n",
    "- **First-stage**: `multi-qa-MiniLM-L6` (bi-encoder)\n",
    "- **Reranking**: `cross-encoder/ms-marco-MiniLM-L-6-v2` (cross-encoder)\n",
    "\n",
    "Let's implement reranking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. PyTerrier: Composable IR Pipelines\n",
    "\n",
    "**PyTerrier** is a Python framework for information retrieval research that makes it easy to build, compose, and compare retrieval systems.\n",
    "\n",
    "#### Core Concept: Everything is a Transformer\n",
    "\n",
    "In PyTerrier, every component is a **transformer**:\n",
    "- Takes queries as input → Returns results as output\n",
    "- Has a standard transform method\n",
    "- Can be combined with other transformers\n",
    "\n",
    "Think of it like Unix pipes for IR\n",
    "\n",
    "#### Pipeline Operators\n",
    "\n",
    "PyTerrier provides powerful operators to compose retrieval systems:\n",
    "\n",
    "**1. Sequential operator >>** (then)\n",
    "\n",
    "Chains transformers in sequence. Example: bm25 >> reranker\n",
    "- Step 1: BM25 retrieves candidates\n",
    "- Step 2: Reranker re-scores those candidates\n",
    "\n",
    "**2. Parallel operator +** (combine)\n",
    "\n",
    "Merges results from multiple retrievers. Example: bm25 + faiss_pt\n",
    "- BM25 retrieves 100 docs\n",
    "- FAISS retrieves 100 docs\n",
    "- Combined: Both sets merged\n",
    "\n",
    "**3. Cutoff operator %** (limit)\n",
    "\n",
    "Limits number of results. Example: bm25 % 100\n",
    "- Only keep top-100 results from BM25\n",
    "\n",
    "#### Building Complex Pipelines\n",
    "\n",
    "You can combine these operators for hybrid retrieval with reranking\n",
    "\n",
    "#### Why PyTerrier?\n",
    "\n",
    "- ✅ Composable: Build complex systems from simple components\n",
    "- ✅ Reproducible: Standard format for sharing experiments\n",
    "- ✅ Comparable: Easy to swap components and compare\n",
    "- ✅ Research-grade: Used in academic IR research\n",
    "- ✅ Pythonic: Works with pandas, sklearn, etc\n",
    "\n",
    "#### In Our Notebook\n",
    "\n",
    "We have wrapped our components as PyTerrier transformers:\n",
    "- bm25 - BM25 retrieval (sparse)\n",
    "- faiss_pt - FAISS retrieval (dense)\n",
    "- pt_reranker - CrossEncoder reranking\n",
    "\n",
    "Now we can compose them in different ways and compare which works best!\n",
    "\n",
    "Let's build some pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. BM25: Traditional Keyword-Based Retrieval\n",
    "\n",
    "Before neural methods, **BM25** (Best Match 25) was (and still is!) the gold standard for information retrieval.\n",
    "\n",
    "### How BM25 Works\n",
    "\n",
    "BM25 is a **sparse retrieval** method that ranks documents based on:\n",
    "- **Term Frequency (TF)**: How often query words appear in the document\n",
    "- **Document Length**: Normalizes for longer documents\n",
    "- **Inverse Document Frequency (IDF)**: Rare words get higher weight\n",
    "\n",
    "**Example Query**: \"how do airplanes fly\"\n",
    "\n",
    "BM25 scores documents by:\n",
    "1. Finding documents containing these words\n",
    "2. Weighing rare words (e.g., \"airplanes\") more than common ones (e.g., \"how\")\n",
    "3. Preferring documents where query terms appear frequently\n",
    "\n",
    "### BM25 vs Dense Retrieval\n",
    "\n",
    "| Aspect | BM25 (Sparse) | Dense (FAISS) |\n",
    "|--------|---------------|---------------|\n",
    "| **Matching** | Exact keyword matching | Semantic similarity |\n",
    "| **Speed** | Very fast | Fast |\n",
    "| **Synonyms** | Misses synonyms | Handles synonyms |\n",
    "| **Exact terms** | Excellent | Can miss exact terms |\n",
    "| **Interpretable** | Yes (can see matched terms) | No (black box) |\n",
    "\n",
    "### Why Still Use BM25?\n",
    "\n",
    "Even with neural methods, BM25 remains valuable:\n",
    "- ✅ Excellent for **exact keyword queries** (e.g., product names, IDs)\n",
    "- ✅ **Interpretable** - you know why a document matched\n",
    "- ✅ **Fast** - no neural model needed\n",
    "- ✅ **Strong baseline** - often competitive with neural methods\n",
    "- ✅ **Complementary** - works great in hybrid with dense retrieval\n",
    "\n",
    "### In This Notebook\n",
    "\n",
    "We'll use **PyTerrier** to:\n",
    "1. Build a BM25 index from our passages\n",
    "2. Retrieve candidates using keyword matching\n",
    "3. Combine with FAISS (hybrid retrieval)\n",
    "4. Apply neural reranking to improve results\n",
    "\n",
    "Let's build our BM25 index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index with PyTerrier\n",
    "print(\"=\" * 70)\n",
    "print(\"BUILDING BM25 INDEX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create documents DataFrame for PyTerrier\n",
    "print(\"\\nPreparing documents for indexing...\")\n",
    "docs_df = pd.DataFrame({\"docno\": passage_ids, \"text\": passages})\n",
    "\n",
    "print(f\"Documents: {len(docs_df):,}\")\n",
    "\n",
    "# Create index\n",
    "print(\"\\nBuilding BM25 index (this may take a few minutes)...\")\n",
    "\n",
    "indexer = pt.DFIndexer(\"./bm25_index\", overwrite=True, verbose=False)\n",
    "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
    "\n",
    "print(\"\\nBM25 index built!\")\n",
    "print(\"Index location: ./bm25_index\")\n",
    "print(\"Retriever ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BM25 retriever with text included (default bm25 does not include text)\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING BM25 RETRIEVER (WITH TEXT)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "class BM25WithText(pt.Transformer):\n",
    "    \"\"\"BM25 retriever that includes passage text in results\"\"\"\n",
    "\n",
    "    def __init__(self, index_ref, passages_dict, wmodel=\"BM25\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index_ref: PyTerrier index reference\n",
    "            passages_dict: Dictionary mapping docno -> text\n",
    "            wmodel: Weighting model (default: BM25)\n",
    "        \"\"\"\n",
    "        self.retriever = pt.BatchRetrieve(index_ref, wmodel=wmodel, verbose=False)\n",
    "        self.passages_dict = passages_dict\n",
    "\n",
    "    def transform(self, queries_df):\n",
    "        \"\"\"Retrieve and add text field\"\"\"\n",
    "        # Get BM25 results\n",
    "        results = self.retriever.transform(queries_df)\n",
    "\n",
    "        # Add text field\n",
    "        results[\"text\"] = results[\"docno\"].map(self.passages_dict)\n",
    "\n",
    "        # Add query field if not present\n",
    "        if \"query\" not in results.columns:\n",
    "            query_map = dict(zip(queries_df[\"qid\"], queries_df[\"query\"]))\n",
    "            results[\"query\"] = results[\"qid\"].map(query_map)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Create passage lookup dictionary\n",
    "passages_dict = dict(zip(passage_ids, passages))\n",
    "\n",
    "# Create BM25 retriever with text\n",
    "bm25 = BM25WithText(index_ref, passages_dict, wmodel=\"BM25\")\n",
    "\n",
    "print(\"BM25 retriever created!\")\n",
    "print(\"Automatically includes passage text in results\")\n",
    "print(\"Ready for use in pipelines and reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTerrier CrossEncoder Reranker\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING NEURAL RERANKER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "class CrossEncoderReranker(pt.Transformer):\n",
    "    \"\"\"PyTerrier transformer for cross-encoder reranking\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", batch_size=32\n",
    "    ):\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.model = CrossEncoder(model_name, max_length=512, device=device)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def transform(self, results_df):\n",
    "        \"\"\"Rerank results using cross-encoder\"\"\"\n",
    "        if len(results_df) == 0:\n",
    "            return results_df\n",
    "\n",
    "        # Handle missing text values\n",
    "        results_df = results_df.copy()\n",
    "        results_df[\"text\"] = results_df[\"text\"].fillna(\n",
    "            \"\"\n",
    "        )  # Replace NaN with empty string\n",
    "        results_df = results_df[results_df[\"text\"] != \"\"]  # Remove rows with no text\n",
    "\n",
    "        if len(results_df) == 0:\n",
    "            return results_df\n",
    "\n",
    "        reranked_results = []\n",
    "\n",
    "        for qid in results_df[\"qid\"].unique():\n",
    "            query_results = results_df[results_df[\"qid\"] == qid].copy()\n",
    "            query_text = query_results.iloc[0][\"query\"]\n",
    "\n",
    "            # Ensure text is string\n",
    "            query_results[\"text\"] = query_results[\"text\"].astype(str)\n",
    "\n",
    "            # Create query-passage pairs\n",
    "            pairs = [[query_text, row[\"text\"]] for _, row in query_results.iterrows()]\n",
    "\n",
    "            # Score with cross-encoder\n",
    "            scores = self.model.predict(pairs, batch_size=self.batch_size)\n",
    "\n",
    "            # Update scores and re-rank\n",
    "            query_results[\"score\"] = scores\n",
    "            query_results = query_results.sort_values(\"score\", ascending=False)\n",
    "            query_results[\"rank\"] = range(len(query_results))\n",
    "\n",
    "            reranked_results.append(query_results)\n",
    "\n",
    "        return pd.concat(reranked_results, ignore_index=True)\n",
    "\n",
    "\n",
    "# Create the reranker\n",
    "pt_reranker = CrossEncoderReranker(\n",
    "    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Reranker loaded!\")\n",
    "print(\"Can process query-passage pairs for accurate scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap FAISS retriever in PyTerrier format\n",
    "print(\"=\" * 70)\n",
    "print(\"Creating FAISS PyTerrier Wrapper\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "class FAISSRetriever(pt.Transformer):\n",
    "    \"\"\"PyTerrier wrapper for FAISS dense retrieval\"\"\"\n",
    "\n",
    "    def __init__(self, index, passages, passage_ids, embedding_model, k=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: FAISS index\n",
    "            passages: List of passage texts\n",
    "            passage_ids: List of passage IDs\n",
    "            embedding_model: SentenceTransformer model for encoding queries\n",
    "            k: Number of results to retrieve\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.passages = passages\n",
    "        self.passage_ids = passage_ids\n",
    "        self.embedding_model = embedding_model\n",
    "        self.k = k\n",
    "\n",
    "    def transform(self, queries_df):\n",
    "        \"\"\"\n",
    "        Transform queries to retrieval results\n",
    "\n",
    "        Args:\n",
    "            queries_df: DataFrame with columns ['qid', 'query']\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns ['qid', 'query', 'docno', 'score', 'rank', 'text']\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        for idx, row in queries_df.iterrows():\n",
    "            qid = row[\"qid\"]\n",
    "            query_text = row[\"query\"]\n",
    "\n",
    "            # Encode query\n",
    "            query_emb = self.embedding_model.encode(\n",
    "                [query_text], normalize_embeddings=True, convert_to_tensor=False\n",
    "            )\n",
    "\n",
    "            # Search FAISS\n",
    "            scores, indices = self.index.search(query_emb.astype(\"float32\"), self.k)\n",
    "\n",
    "            # Format results for PyTerrier\n",
    "            for rank, (score, doc_idx) in enumerate(zip(scores[0], indices[0])):\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"qid\": qid,\n",
    "                        \"query\": query_text,\n",
    "                        \"docno\": self.passage_ids[doc_idx],\n",
    "                        \"score\": float(score),\n",
    "                        \"rank\": rank,\n",
    "                        \"text\": self.passages[doc_idx],  # Needed for reranker\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Create FAISS retriever\n",
    "faiss_pt = FAISSRetriever(\n",
    "    index=index,\n",
    "    passages=passages,\n",
    "    passage_ids=passage_ids,\n",
    "    embedding_model=embedding_model,\n",
    "    k=100,\n",
    ")\n",
    "\n",
    "print(\"\\nFAISS PyTerrier wrapper created!\")\n",
    "print(\"Will retrieve top-{100} results per query\")\n",
    "print(\"Can now be used in PyTerrier pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed hybrid pipeline with score normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class ScoreNormalizer(pt.Transformer):\n",
    "    \"\"\"Normalize scores to 0-1 range per query\"\"\"\n",
    "\n",
    "    def transform(self, results_df):\n",
    "        if len(results_df) == 0:\n",
    "            return results_df\n",
    "\n",
    "        results_df = results_df.copy()\n",
    "\n",
    "        # Normalize scores per query\n",
    "        for qid in results_df[\"qid\"].unique():\n",
    "            mask = results_df[\"qid\"] == qid\n",
    "            scores = results_df.loc[mask, \"score\"].values.reshape(-1, 1)\n",
    "\n",
    "            if len(scores) > 0:\n",
    "                scaler = MinMaxScaler()\n",
    "                normalized = scaler.fit_transform(scores).flatten()\n",
    "                results_df.loc[mask, \"score\"] = normalized\n",
    "\n",
    "        return results_df\n",
    "\n",
    "\n",
    "# Create normalizer\n",
    "score_normalizer = ScoreNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all retrieval pipelines for comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 2: Defining Retrieval Pipelines\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pipeline 1: Sparse only (BM25)\n",
    "pipeline_sparse = bm25 % 100\n",
    "\n",
    "# Pipeline 2: Dense only (FAISS)\n",
    "pipeline_dense = faiss_pt\n",
    "\n",
    "# Pipeline 3: Sparse + Reranking (BM25 → CrossEncoder)\n",
    "pipeline_sparse_rerank = (bm25 % 100) >> pt_reranker\n",
    "\n",
    "# Pipeline 4: Dense + Reranking (FAISS → CrossEncoder)\n",
    "pipeline_dense_rerank = faiss_pt >> pt_reranker\n",
    "\n",
    "# Fixed hybrid: normalize both before combining\n",
    "pipeline_hybrid_rerank = (\n",
    "    ((bm25 % 100) >> score_normalizer) + (faiss_pt >> score_normalizer)\n",
    ") >> pt_reranker\n",
    "\n",
    "\n",
    "# Store all pipelines with descriptive names\n",
    "pipelines = {\n",
    "    \"1. Sparse (BM25)\": pipeline_sparse,\n",
    "    \"2. Dense (FAISS)\": pipeline_dense,\n",
    "    \"3. Sparse + Rerank\": pipeline_sparse_rerank,\n",
    "    \"4. Dense + Rerank\": pipeline_dense_rerank,\n",
    "    \"5. Hybrid + Rerank\": pipeline_hybrid_rerank,\n",
    "}\n",
    "\n",
    "print(\"\\nAll pipelines defined!\\n\")\n",
    "print(\"Pipeline Overview:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    if name == \"1. Sparse (BM25)\":\n",
    "        print(\"Traditional keyword matching\")\n",
    "        print(\"Fast, interpretable\")\n",
    "        print(\"Good for exact term matches\")\n",
    "    elif name == \"2. Dense (FAISS)\":\n",
    "        print(\"Neural semantic search\")\n",
    "        print(\"Good for paraphrases\")\n",
    "        print(\"Understands meaning, not just keywords\")\n",
    "    elif name == \"3. Sparse + Rerank\":\n",
    "        print(\"BM25 retrieves candidates\")\n",
    "        print(\"CrossEncoder reranks for accuracy\")\n",
    "        print(\"Best of keyword + neural\")\n",
    "    elif name == \"4. Dense + Rerank\":\n",
    "        print(\"FAISS retrieves semantic matches\")\n",
    "        print(\"CrossEncoder refines ranking\")\n",
    "        print(\"Fully neural pipeline\")\n",
    "    elif name == \"5. Hybrid + Rerank\":\n",
    "        print(\"Combines BM25 + FAISS results\")\n",
    "        print(\"CrossEncoder reranks merged results\")\n",
    "        print(\"Most comprehensive approach\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"The >> operator chains transformers sequentially\")\n",
    "print(\"The + operator combines results from multiple retrievers\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess queries to avoid PyTerrier parsing errors\n",
    "print(\"Preprocessing queries for PyTerrier...\")\n",
    "\n",
    "# Prepare test queries (subset for efficiency)\n",
    "num_test_queries = 100\n",
    "test_queries_df = pd.DataFrame(\n",
    "    {\"qid\": query_ids[:num_test_queries], \"query\": queries[:num_test_queries]}\n",
    ")\n",
    "\n",
    "\n",
    "def clean_query_for_pyterrier(query):\n",
    "    \"\"\"Remove or escape special characters that confuse PyTerrier's query parser\"\"\"\n",
    "    import re\n",
    "\n",
    "    # Remove problematic characters\n",
    "    query = query.replace(\"'\", \"\")  # Remove apostrophes\n",
    "    query = query.replace('\"', \"\")  # Remove quotes\n",
    "    query = re.sub(r\"[^\\w\\s]\", \" \", query)  # Remove other special chars\n",
    "    query = \" \".join(query.split())  # Normalize whitespace\n",
    "    return query\n",
    "\n",
    "\n",
    "# Clean the queries\n",
    "test_queries_df[\"query\"] = test_queries_df[\"query\"].apply(clean_query_for_pyterrier)\n",
    "\n",
    "print(\"Queries cleaned for PyTerrier\")\n",
    "print(\"\\nExample cleaned query:\")\n",
    "print(f\"Original: {queries[57]}\")\n",
    "print(\n",
    "    f\"Cleaned: {test_queries_df[test_queries_df['qid'] == 'q_57']['query'].values[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all pipelines and collect results\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 4: Running All Pipelines\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTesting on {len(test_queries_df)} queries\\n\")\n",
    "\n",
    "# Run each pipeline and store results\n",
    "results_dict = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"Running: {name}\")\n",
    "    # Transform queries through pipeline\n",
    "    results = pipeline.transform(test_queries_df)\n",
    "    results_dict[name] = results\n",
    "    print(\"Completed!\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"All pipelines executed successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluation Metrics for Information Retrieval\n",
    "\n",
    "How do we measure if our retrieval system is good? We use several metrics:\n",
    "\n",
    "### 1. Mean Reciprocal Rank (MRR)\n",
    "\n",
    "**What it measures**: Position of the first relevant result\n",
    "\n",
    "**Formula**:\n",
    "$$MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}$$\n",
    "\n",
    "Where:\n",
    "- $|Q|$ = number of queries\n",
    "- $rank_i$ = position of first relevant document for query $i$\n",
    "\n",
    "**Example**:\n",
    "- Query 1: First relevant at rank 1 → Score = 1.0\n",
    "- Query 2: First relevant at rank 3 → Score = 0.333\n",
    "- Query 3: First relevant at rank 10 → Score = 0.1\n",
    "- MRR = (1.0 + 0.333 + 0.1) / 3 = 0.478\n",
    "\n",
    "**Range**: 0 to 1 (higher is better)\n",
    "\n",
    "### 2. Recall@K\n",
    "\n",
    "**What it measures**: How many relevant documents are in top-K results\n",
    "\n",
    "**Formula:** \n",
    "\n",
    "$$Recall@K = \\frac{\\text{Relevant docs in top-K}}{\\text{Total relevant docs}}$$\n",
    "\n",
    "**Example**: 5 relevant docs total, 3 in top-10 → Recall@10 = 0.6\n",
    "\n",
    "### 3. Precision@K\n",
    "\n",
    "**What it measures**: What fraction of top-K results are relevant\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "$$Precision@K = \\frac{\\text{Relevant docs in top-K}}{K}$$\n",
    "\n",
    "### 4. Normalized Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "**What it measures**: Quality considering position (top results matter more)\n",
    "\n",
    "**Formula**:\n",
    "\n",
    "$$nDCG@K = \\frac{DCG@K}{IDCG@K}$$\n",
    "\n",
    "Where DCG (Discounted Cumulative Gain) is:\n",
    "\n",
    "$$DCG@K = \\sum_{i=1}^{K} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}$$\n",
    "\n",
    "And $IDCG@K$ is the DCG of the ideal ranking (best possible ordering).\n",
    "\n",
    "**Why it's better**: Rewards putting relevant docs at top positions\n",
    "\n",
    "**Range**: 0 to 1 (higher is better)\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "- **MRR**: Single relevant answer (e.g., question answering)\n",
    "- **Recall@K**: Finding all relevant items (e.g., search engines)\n",
    "- **nDCG**: Ranking quality matters (e.g., recommendation systems)\n",
    "\n",
    "For MS MARCO, **MRR@10** is the official metric! Let's calculate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics using our extracted qrels\n",
    "print(\"Evaluation Metrics Using Qrels\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def calculate_mrr_with_qrels(query_ids, retrieved_doc_ids, qrels, k=10):\n",
    "    \"\"\"Calculate Mean Reciprocal Rank@k using qrels\"\"\"\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for qid, retrieved in zip(query_ids, retrieved_doc_ids):\n",
    "        # Get relevant docs for this query\n",
    "        relevant_docs = set(qrels.get(qid, {}).keys())\n",
    "\n",
    "        if not relevant_docs:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Find rank of first relevant document\n",
    "        for rank, doc_id in enumerate(retrieved[:k], 1):\n",
    "            if doc_id in relevant_docs:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "\n",
    "def calculate_recall_at_k_with_qrels(query_ids, retrieved_doc_ids, qrels, k=10):\n",
    "    \"\"\"Calculate Recall@k using qrels\"\"\"\n",
    "    recalls = []\n",
    "\n",
    "    for qid, retrieved in zip(query_ids, retrieved_doc_ids):\n",
    "        relevant_docs = set(qrels.get(qid, {}).keys())\n",
    "\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        retrieved_relevant = len(set(retrieved[:k]) & relevant_docs)\n",
    "        recall = retrieved_relevant / len(relevant_docs)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "\n",
    "def calculate_precision_at_k_with_qrels(query_ids, retrieved_doc_ids, qrels, k=10):\n",
    "    \"\"\"Calculate Precision@k using qrels\"\"\"\n",
    "    precisions = []\n",
    "\n",
    "    for qid, retrieved in zip(query_ids, retrieved_doc_ids):\n",
    "        relevant_docs = set(qrels.get(qid, {}).keys())\n",
    "\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        retrieved_relevant = len(set(retrieved[:k]) & relevant_docs)\n",
    "        precision = retrieved_relevant / k\n",
    "        precisions.append(precision)\n",
    "\n",
    "    return np.mean(precisions) if precisions else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all pipelines\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 5: Evaluating All Pipelines\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Prepare retrieved doc IDs for evaluation\n",
    "def prepare_retrieved_for_eval(results_df, query_ids_list):\n",
    "    \"\"\"Convert results DataFrame to list of retrieved doc IDs per query\"\"\"\n",
    "    retrieved = []\n",
    "    for qid in query_ids_list:\n",
    "        query_results = results_df[results_df[\"qid\"] == qid].sort_values(\n",
    "            \"score\", ascending=False\n",
    "        )\n",
    "        retrieved.append(query_results[\"docno\"].tolist())\n",
    "    return retrieved\n",
    "\n",
    "\n",
    "# Get test query IDs\n",
    "test_qids = query_ids[:num_test_queries]\n",
    "\n",
    "# Calculate metrics for each pipeline\n",
    "k_values = [1, 5, 10]\n",
    "all_metrics = {}\n",
    "\n",
    "print(f\"\\nCalculating metrics for {len(pipelines)} pipelines...\\n\")\n",
    "\n",
    "for pipeline_name, results_df in results_dict.items():\n",
    "    print(f\"Evaluating: {pipeline_name}\")\n",
    "\n",
    "    # Prepare retrieved documents\n",
    "    retrieved = prepare_retrieved_for_eval(results_df, test_qids)\n",
    "\n",
    "    # Calculate metrics at different K\n",
    "    metrics = {}\n",
    "    for k in k_values:\n",
    "        mrr = calculate_mrr_with_qrels(test_qids, retrieved, qrels, k)\n",
    "        recall = calculate_recall_at_k_with_qrels(test_qids, retrieved, qrels, k)\n",
    "        precision = calculate_precision_at_k_with_qrels(test_qids, retrieved, qrels, k)\n",
    "\n",
    "        metrics[f\"MRR@{k}\"] = mrr\n",
    "        metrics[f\"Recall@{k}\"] = recall\n",
    "        metrics[f\"Precision@{k}\"] = precision\n",
    "\n",
    "    all_metrics[pipeline_name] = metrics\n",
    "    print(\"Metrics calculated\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Evaluation complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display comprehensive comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL COMPARISON: All Pipelines\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create comparison table\n",
    "# Build comparison DataFrame\n",
    "comparison_data = []\n",
    "for pipeline_name, metrics in all_metrics.items():\n",
    "    row = {\"Pipeline\": pipeline_name}\n",
    "    row.update(metrics)\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display table\n",
    "print(\"\\nMETRICS COMPARISON TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Highlight best performers\n",
    "print(\"\\nBEST PERFORMERS:\")\n",
    "print(\"-\" * 70)\n",
    "for metric in [\"MRR@10\", \"Recall@10\", \"Precision@10\"]:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    best_pipeline = comparison_df.loc[best_idx, \"Pipeline\"]\n",
    "    best_value = comparison_df.loc[best_idx, metric]\n",
    "    print(f\"{metric:15s}: {best_pipeline:25s} ({best_value:.4f})\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle(\"Complete Pipeline Comparison\", fontsize=18, fontweight=\"bold\", y=0.995)\n",
    "\n",
    "pipeline_names = [\n",
    "    p.split(\". \")[1] if \". \" in p else p for p in comparison_df[\"Pipeline\"]\n",
    "]\n",
    "colors = [\"steelblue\", \"coral\", \"lightgreen\", \"orange\", \"purple\"]\n",
    "\n",
    "# Plot each metric\n",
    "metrics_to_plot = [\"MRR@1\", \"MRR@5\", \"MRR@10\", \"Recall@10\", \"Precision@10\"]\n",
    "positions = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1)]\n",
    "\n",
    "for metric, pos in zip(metrics_to_plot, positions):\n",
    "    ax = axes[pos]\n",
    "    values = comparison_df[metric].values\n",
    "\n",
    "    bars = ax.bar(\n",
    "        range(len(values)), values, color=colors, alpha=0.8, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(f\"{metric} Comparison\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_xticks(range(len(pipeline_names)))\n",
    "    ax.set_xticklabels(pipeline_names, rotation=45, ha=\"right\", fontsize=10)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    ax.set_ylim(0, max(values) * 1.15)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "# Summary subplot\n",
    "ax_summary = axes[1, 2]\n",
    "ax_summary.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways & What You've Learned\n",
    "\n",
    "Congratulations! You've built a complete modern information retrieval system. Here's what you now understand:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **The Two-Stage Retrieval Paradigm**\n",
    "\n",
    "**This is how production search engines work:**\n",
    "- **Stage 1**: Fast retrieval from millions (BM25/FAISS) → Top 100-1000 candidates\n",
    "- **Stage 2**: Accurate reranking (CrossEncoder) → Final top 10 results\n",
    "- **Why?** Balance speed (stage 1) with accuracy (stage 2)\n",
    "\n",
    "**Real-world impact:** Google, Bing, Amazon all use this pattern!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Sparse vs Dense Retrieval**\n",
    "\n",
    "**Sparse (BM25):**\n",
    "- ✅ Excellent for exact keyword matching\n",
    "- ✅ Fast and interpretable\n",
    "- ❌ Misses synonyms and paraphrases\n",
    "\n",
    "**Dense (FAISS):**\n",
    "- ✅ Semantic understanding\n",
    "- ✅ Handles synonyms naturally\n",
    "- ❌ Can miss exact term matches\n",
    "\n",
    "**Hybrid (BM25 + FAISS):**\n",
    "- ✅ Best of both worlds\n",
    "- ✅ More robust across query types\n",
    "- ⚠️ Requires careful score normalization\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Neural Reranking is Powerful**\n",
    "\n",
    "**Why CrossEncoders are better for reranking:**\n",
    "- See query + passage **together** (not separately)\n",
    "- Capture subtle relevance signals\n",
    "- Significantly improve top results\n",
    "\n",
    "**The trade-off:**\n",
    "- Much slower than first-stage retrieval\n",
    "- Only practical for top-K candidates (K = 100-1000)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **PyTerrier: The Research Tool**\n",
    "\n",
    "**What you learned:**\n",
    "- `>>` chains transformers sequentially\n",
    "- `+` combines results from multiple retrievers\n",
    "- `%` limits number of results\n",
    "- Build complex pipelines from simple components\n",
    "\n",
    "**Why it matters:** Industry-standard approach for IR research and experimentation\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Evaluation Metrics Matter**\n",
    "\n",
    "**Different metrics measure different things:**\n",
    "- **MRR@K**: Where is the first relevant result?\n",
    "- **Recall@K**: Did we find the relevant documents?\n",
    "- **Precision@K**: What fraction of results are relevant?\n",
    "\n",
    "**Key insight:** Always evaluate on real relevance judgments (qrels)!\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Embeddings are Powerful Representations**\n",
    "\n",
    "**What makes dense embeddings work:**\n",
    "- Neural networks learn semantic meaning\n",
    "- Similar concepts → Similar vectors\n",
    "- Works across languages and phrasings\n",
    "\n",
    "**Limitation:** Still need sparse methods for exact matches!\n",
    "\n",
    "---\n",
    "\n",
    "## What You Can Do Next\n",
    "\n",
    "### Immediate Extensions:\n",
    "1. Try different embedding models (larger = better quality, slower)\n",
    "2. Experiment with different K values (top-50 vs top-200)\n",
    "3. Add query expansion techniques\n",
    "4. Try other PyTerrier components\n",
    "\n",
    "### Real-World Applications:\n",
    "- **Search Engines**: Document retrieval systems\n",
    "- **RAG Systems**: Retrieval for LLMs (ChatGPT-style apps)\n",
    "- **Recommendation**: Similar item finding\n",
    "- **Customer Support**: Finding relevant help articles\n",
    "- **Academic Search**: Research paper retrieval\n",
    "\n",
    "### Advanced Topics to Explore:\n",
    "- ColBERT (late interaction models)\n",
    "- Cross-lingual retrieval\n",
    "- Multi-modal search (text + images)\n",
    "- Learning to rank (LTR)\n",
    "- Query understanding and expansion\n",
    "\n",
    "---\n",
    "\n",
    "Keep experimenting, and remember: **The best IR system depends on your specific use case**. Sometimes BM25 is enough, sometimes you need the full hybrid+reranking pipeline. Always evaluate on real data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
